---
title: 空品預報時距之延長
tags: GFS WRF CMAQ forecast
layout: article
aside:
  toc: true
sidebar:
  nav: layouts
date:  2022-12-06
modify_date: 2022-12-12 11:08:24
---

## 背景

- 目前全球預報產品的時距(leading time)、[GFS][GFS]事實上是有348個小時，即為16天整([目前使用前5天][fcst5])。空氣品質[CAMS][CAMS]有120小時即為5天、[WACCM][WACCM]則由昨天開始到未來9天共10天預報。
- 延長時距的挑戰在於
  - [電腦計算時間](#os控制之背景同步執行)延長一倍，原來5天預報還可以壓縮在6個小時之內，早晨上班還可以看到更新之預報，延長1倍到12小時似乎就失去每天預報的價值。
  - [各領域風場的相依性與整合](#風場的整合)：雖然每一層都已開啟[FDDA][FDDA]，然而因時間延長一倍、風場非線性發展的可能性增加，上層模擬結果與下層有可能發生嚴重落差，需加強各層之間的連結。
  - [東亞邊界檔案的整併](#waccm數據之應用)：目前看來WACCM模擬似乎有低估的情形，而CAMS只有5天預報(嚴格講是昨天開始的5天、就是4天)，如何應用?
  - [排放量檔案擴大、跨月整合](#排放量預備程式之修正)：原本逐月排放量檔案已延常到下月初，然而並未有10天這麼長，終究還是要解決跨月的整併問題。
- GFS數據
  - 下載的方式也經過比較（[[2022-12-01-wget_vs_curl]][^1]）
  - 其他GFS討論([[2022-08-10-GFStoWRF]][^2])、數據展示([[2022-08-05-earth_gfs]][^3])

## 風場的整合

### 延長時距遭遇的困難

- 原來1、2層雙向巢狀網格($gfs/tw_CWBWRF_45k)、第3層獨立邊界(TWEPA_3k)的作法，導致第5天在第2層邊界上發生質量不守恆的情形，造成CMAQ跳出。
- 3層雙向巢狀網格的做法會需要6個小時的計算時間，為不可行方案。

### ndown方案

- 藉由`ndown.exe`程式讀取1,2層雙向巢狀網格中第2層wrfout檔案，從當中切割出第3層的邊界條件，取代第3層單獨的`real.exe`作業。
- 作法詳見[ndown][ndown]
- 基本設定：因為需要起訖時間，因此還是再做一遍。

```bash
kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
$ cat ndown.cs

today=$(date -d -0day +%Y%m%d)
yestd=$(date -d -1day +%Y%m%d)
yesty=$(date -d -1day +%Y)
gfs=/nas1/backup/data/NOAA/NCEP/GFS/YYYY
cmaq=/home/cmaqruns/2022fcst
fcst=/nas2/cmaqruns/2022fcst
BEGD=$(date -d "$today -0days" +%Y-%m-%d)
ENDD=$(date -d "$BEGD  +11days" +%Y-%m-%d)

DOM=( 'CWBWRF_45k' 'SECN_9k' 'TWEPA_3k' 'tw_CWBWRF_45k' 'nests3')
MPI=( '-f machinefile -np 200' '-f machinefile -np 196' '-f machinefile -np 140' '-f machinefile -np 120' '-f machinefile -np 120')


yea1=$(echo $BEGD|cut -d'-' -f1);mon1=$(echo $BEGD|cut -d'-' -f2);day1=$(echo $BEGD|cut -d'-' -f3)
yea2=$(echo $ENDD|cut -d'-' -f1);mon2=$(echo $ENDD|cut -d'-' -f2);day2=$(echo $ENDD|cut -d'-' -f3)
dates=()
for id in {0..11};do
  dates=( ${dates[@]} $(date -d "$BEGD +${id}days" +%Y-%m-%d) )
done
yea1=$(echo $BEGD|cut -d'-' -f1);mon1=$(echo $BEGD|cut -d'-' -f2);day1=$(echo $BEGD|cut -d'-' -f3)
yea2=$(echo $ENDD|cut -d'-' -f1);mon2=$(echo $ENDD|cut -d'-' -f2);day2=$(echo $ENDD|cut -d'-' -f3)
```

- 製作第2,3層`real.exe`所需的namelist.input

```bash
i=2
cd $gfs/${DOM[$i]}/ndown
cp namelist.input23_loop namelist.input
  for cmd in "s/SYEA/$yea1/g" "s/SMON/$mon1/g" "s/SDAY/$day1/g" \
             "s/EYEA/$yea2/g" "s/EMON/$mon2/g" "s/EDAY/$day2/g" ;do
    sed -i $cmd namelist.input
  done
```

- 清除檔案並建立新的連結

```bash
for hd in metoa_em wrf;do if compgen -G "${hd}*" > /dev/null; then rm -f ${hd}*;fi;done

for d in 2 3;do
  dd=$(( $d - 1 ))
  for id in {0..11};do
    for j in $(ls ../../met_em.d0${d}.${dates[$id]}_*);do
      k=${j/d0${d}/d0${dd}}
      l=${k/..\/..\//}
      m=${l/met_/metoa_};ln -s $j $m;done;done;done
```

- 執行大陸東南與台灣等2層的`real.exe`
  - 大陸東南重複執行了，但似乎也沒有辦法減省。
  - 第3層(台灣)單獨執行`real.exe`的結果不能用來執行ndown.exe

```bash
LD_LIBRARY_PATH=/nas1/WRF4.0/WRFv4.3/WRFV4/LIBRARIES/lib:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/compiler/lib/intel64_lin:/opt/mpich/mpich-3.4.2-icc/lib /opt/mpich/mpich-3.4.2-icc/bin/mpirun ${MPI[$i]} /nas1/WRF4.0/WRFv4.3/WRFV4/main/real.exe >& /dev/null
```

- 準備執行ndown.exe
  - 將工作目錄($gfs/TWEPA_3k/ndown)下`real.exe`結果中第2層的起始檔(wrfinput_d02)，更名為wrfndi_d02。ndown.exe會產生一個新的wrfinput_d02
  - 將tw_CWBWRF_45k執行結果的第2層(SECN_9k範圍)連結到工作目錄
  - 時間間距改為1小時，以配合wrfout的時距

```bash
mv wrfinput_d02 wrfndi_d02

for id in {0..10};do ln -sf $gfs/${DOM[3]}/wrfout_d02_${dates[$id]}_00:00:00 wrfout_d01_${dates[$id]}_00:00:00;done

sed -i 's/interval_seconds                    = 10800/interval_seconds                    = 3600/g' namelist.input
```

- 執行ndown.exe
  - ndown.exe執行還算快速，用10個核心就夠了
  - 將目錄下的執行結果移動到TWEPA_3k目錄，以備第3層(單層)wrf之執行。此處($gfs/TWEPA_3k/ndown)的第2層即為$gfs/TWEPA_3k目錄的第1層
  - 需要移動的檔案包括初始檔`wrfinput`、邊界檔`wrfbdy`(此二者經ndown.exe所產生、覆蓋原來`real.exe`的結果)

```bash
#ndown.exe is intel version
LD_LIBRARY_PATH=/nas1/WRF4.0/WRFv4.3/WRFV4/LIBRARIES/lib:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/compiler/lib/intel64_lin:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/mpi/intel64/lib:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/mpi/intel64/lib/release:/opt/intel/compilers_and_libraries_2020.0.166/linux/mpi/intel64/libfabric/lib /opt/intel_f/compilers_and_libraries_2020.0.166/linux/mpi/intel64/bin/mpirun -np 10 /nas1/WRF4.0/WRFv4.3/WRFV4/main/ndown.exe >& /dev/null

## restore the real and ndown results
cd $gfs/${DOM[$i]}
for f in wrfinput wrfbdy wrffdda wrflowinp;do
  mv ndown/${f}_d02 ${f}_d01
done
```

### ndown.cs下載點

- {% include download.html content="由大陸東南wrfout結果切割下層邊界條件之作業腳本[ndown.cs][ndowncs]" %}

## OS控制之背景同步執行

### WPS

- 下載gfs檔案後，序列化執行WPS之[ungrib][ungrib]與[metgrid][metgrid]花費將近40分鐘，如果分成各個gfs檔案個別(平行)轉檔，只需要5分鐘。
- 為避免[ungrib][ungrib]錯亂，每一個gfs檔案建立自己的目錄。先將WPS目錄下相關檔案都見好連結，每天的gfs檔名又是固定的，只需改變namelist.wps內的起迄時間即可。

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
$ cat par_UGB.cs
gfs=/nas1/backup/data/NOAA/NCEP/GFS/YYYY
BEGD=$(date -d "$today -1days" +%Y-%m-%d)

for ((i=0;i <= 312; i+=3));do
  iii=$(printf "%03d" $i)
  NOWD=$(date -d "$BEGD +$(( $i + 6 ))hour" +%Y-%m-%d )
  hh=$(date -d "$BEGD +$(( $i + 6 ))hour" +%H )
  mkdir -p ${gfs}/f$iii
  cd ${gfs}/f$iii
  ./link_grib.csh gfs*
  cp ../namelist.wps_loop namelist.wps
  for cmd in 's/BEGD/'$NOWD'/g' 's/ENDD/'$NOWD'/g' 's/HH/'$hh'/g';do sed -ie $cmd namelist.wps;done

## ungrib and metgrid
  ~/bin/sub ../UGB2
done

# first time, link the WPS relatives
# for i in $(ls gfs*);do j=$(echo $i|cut -d'.' -f5);for k in $(ls|grep -v gfs|grep -v met_em|grep -v FILE|grep -v GRIBF|grep -v ^f|grep -v namelist);do cd $j;ln -s ../$k .;cd ..;done;done
```

- 執行[ungrib][ungrib]及[metgrid][metgrid]結束後，順便將結果移動到根目錄
- UGB2內之指令為序列執行

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat UGB2
LD_LIBRARY_PATH=/nas1/WRF4.0/WRFv4.3/WRFV4/LIBRARIES/lib:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/compiler/lib/intel64_lin /nas1/WRF4.0/WRF_chem/WPS/ungrib/src/ungrib.exe >& /dev/null
LD_LIBRARY_PATH=/nas1/WRF4.0/WRFv4.3/WRFV4/LIBRARIES/lib:/opt/intel_f/compilers_and_libraries_2020.0.166/linux/compiler/lib/intel64_lin /nas1/WRF4.0/WRFv4.3/WPS/metgrid/src/metgrid.exe >& /dev/null
mv met_em* ..
```

- 下載(序列)、轉檔(平行)整體串連
- 趁下載的時間同步執行WPS程式

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat fcst.cs
...
# 執行gfs檔案下載
# execute ungrib and metgrid in background
for ((i=0;i <= 312; i+=3));do
  iii=$(printf "%03d" $i)
  file=gfs.t${BH}z.pgrb2.1p00.f$iii
  if [ -e $file ];then rm $file;fi
  while [ 1 ]; do
  $wget --no-check-certificate -q --retry-connrefused --waitretry=3 --random-wait \
        --read-timeout=20 --timeout=15 -t 10 --continue $root$dir$file
  if [ $? = 0 ]; then break; fi
  sleep 5
  done

  NOWD=$(date -d "$BEGD +$(( $i + 10#$BH ))hour" +%Y-%m-%d )
  hh=$(date -d "$BEGD +$(( $i + 10#$BH ))hour" +%H )
  mkdir -p ${gfs}/f$iii
  cd ${gfs}/f$iii
  ./link_grib.csh gfs*
  cp ../namelist.wps_loop namelist.wps
  for cmd in 's/BEGD/'$NOWD'/g' 's/ENDD/'$NOWD'/g' 's/HH/'$hh'/g';do sed -ie $cmd namelist.wps;done
  ~/bin/sub ../UGB2
  cd $gfs
done
...
```

- 背景執行`metgrid.exe`需要`while指令`來確認所有執行檔真的結束了，才能啟動`real.exe`和`wrf.exe`。
- 這個工作由`wait_exe`來執行(詳[以下說明](#確認執行完成之小工具wait_exe))

### 排放量預備程式之同步執行

- 由於排放量與風場是互相獨立的，可以提前在風場模式之前，趁該等程式執行時，同步執行排放量預備程式。
- 見[排放量預備程式之修正](#排放量預備程式之修正)

### `run_mcip`之背景運作

- `run_mcip`雖然不會花太多時間，但加上後處理，3各領域共計約20分鐘，背景運作後這些時間都可以節省下來。
- 但要注意的是後處理，不論`add_firstHr.sh`或者是`brk_day2.cs`都是bash的腳本，需要啟動`/usr/bin/sh`來執行bash的批次檔。

```bash
kuang@master /nas2/cmaqruns/2022fcst
$ tail -n20 run_mcip_DM.csh
mpirun -np $NP $ProgDir/${PROG}.exe #  >& /dev/null
if ( $status == 0 ) then
  rm fort.*
# add first hour
  /usr/bin/sh ~/bin/add_firstHr.sh
  /usr/bin/sh ~/bin/brk_day2.cs METBDY3D.nc
  if ( $DM == 'grid45' ) then
    /usr/bin/ncks -O  -d VAR,0 -v TFLAG,DENS METCRO3D.nc METCRO3D.DENS
    /usr/bin/sh ~/bin/brk_day2.cs METCRO3D.DENS >&/dev/null
  endif
  exit 0
else
  echo "Error running $PROG"
  exit 1
endif
```

- 如此`run_mcip`就可以獨立出來在背景執行了
- 執行下一個`real.exe`(或`ndown.exe`)與`wrf.exe`時，同步執行`run_mcip`

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat fcst.cs
...
  # link the wrfout's and execute mcip(in the background)
  if [ $i == 3 ];then
    for d in 1 2;do
      j=$(( $d - 1))
      for f in {0..10};do
        wrfo=wrfout_d0${d}_${dates[$f]}_00:00:00
        nc1=$gfs/${DOM[$i]}/$wrfo
        if [ -e $nc1 ];then
          wrfo2=${wrfo/d0${d}/d01}
          nc2=$gfs/${DOM[$j]}/$wrfo2
          if [ -e $nc2 ];then rm $nc2;fi
          ln -sf $nc1 $nc2
        fi
      done
  #   mcip
      cd $cmaq/data/wrfout
      for f in {0..10};do nc=$gfs/${DOM[$j]}/wrfout_d01_${dates[$f]}_00:00:00;ln -sf $nc wrfout_d0${d}_$f;done
      cd $fcst;~/bin/sub csh run_mcip_DM.csh ${GRD[$j]} 10 >&/dev/null
    done
  else
  #   mcip i=2,d=3
      cd $cmaq/data/wrfout;j=$i;d=3
      for f in {0..10};do nc=$gfs/${DOM[$j]}/wrfout_d01_${dates[$f]}_00:00:00;ln -sf $nc wrfout_d0${d}_$f;done
      cd $fcst;~/bin/sub csh run_mcip_DM.csh ${GRD[$j]} 10 >&/dev/null
  fi
...
```

### `combine.sh`的背景運作

- 重新整理運作邏輯，趁著切割下一層BC與IC時，讓`combine.sh`單獨在背景運作。
- 第3層因為不必執行`run_bcon`與`run_icon`，需確認`combine.sh`確實完成了，再執行下一個動作(`cmaq_json`)，使用[wait_exe](#確認執行完成之小工具wait_exe)。

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat fcst.cs
...
  csh ./run.cctm.${ii}.csh

  # combine PM's
  for id in {0..9};do
    nc=$fcst/${GRD[$i]}/cctm.fcst/daily/CCTM_ACONC_v532_intel_${DOM[$i]}_${datep[$id]}.nc
    ~/bin/sub $fcst/combine.sh $nc
  done
  if [[ $i < 2 ]];then
  # nest down BCON and ICON
    j=$(( $i + 1))
...
 ~/bin/wait_exe combine #make sure all combine executions are finished
...
```

### [cmaq_json][cmaq_json]的同步執行

- CMAQ轉成json檔是逐日進行的，除了複製檔案到imac之外，並沒有不能同步運作的理由。
- 所以只要將複製的動作分散在各日的python檔內執行即可。這就是`cmaq_jsonByDay.py`的特點。

```python
$ diff cmaq_json3.py cmaq_jsonByDay.py
45c45
< nd=10
---
> nd=1
142a143,145
>     mac=pwd.replace('nas1','home/kuang/mac')
>     os.system('mkdir -p '+mac+dir)
>     os.system('cp '+fnameO+' '+fnameO.replace('nas1','home/kuang/mac'))
178a182,184
>       mac=pwd.replace('nas1','home/kuang/mac')
>       os.system('mkdir -p '+mac+dir)
>       os.system('cp '+fnameO+' '+fnameO.replace('nas1','home/kuang/mac'))
202a209,211
>     mac=pwd.replace('nas1','home/kuang/mac')
>     os.system('mkdir -p '+mac+dir)
>     os.system('cp '+fnameO+' '+fnameO.replace('nas1','home/kuang/mac'))
255a265,268
>
>       mac=pwd.replace('nas1','home/kuang/mac')
>       os.system('mkdir -p '+mac+dir)
>       os.system('cp '+fnameO+' '+fnameO.replace('nas1','home/kuang/mac'))
```

- 執行完cmaq_json整個預報程序就完成了(不需再進行結束之確認)

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat fcst.cs
...
  r=${RES[$i]}
  cd /nas1/Data/javascripts/D3js/earthFcst$r/public/data/weather/current
  for id in {0..9};do ~/bin/sub ./cmaq_jsonByDay.py ${dates[$id]};done
```

### 確認執行完成之小工具wait_exe

```bash
#$ cat ~/bin/wait_exe
EXE=$1
while true;do
  n=$(ps -ef|grep ${EXE}|wc -l)
  if [ $n -lt 2 ];then
    break
  else
    sleep 1
  fi
done
```

## 邊界條件之修正

### [WACCM][WACCM]數據之應用

- 詳見[WACCM模式結果之下載、讀取及應用][w1]及[使用WACCM全球預報作為東亞邊界條件][w2]。
- 因為[WACCM][WACCM]數據檔案非常龐大，下載與分析需要5～6 小時，必須提前、另機、平行運作。
- 合併結果將覆蓋`$fcst/grid$res/bcon/BCON_today_CWBWRF_45k`，併入CMAQ的模擬程序。
- BCON檔案的圖像化(2維xz,yz立面濃度檔轉成水平方向)，可以使用[bcon2icon.py](https://sinotec2.github.io/FAQ/2022/10/26/bcon2icon.html)，再套用[VERDI](https://sinotec2.github.io/Focus-on-Air-Quality/utilities/Graphics/VERDI/VERDI_Guide/)。
- [WACCM][WACCM]數據顯示有明顯低估的情形。最後選擇放棄使用。

### [CAMS][CAMS]數據之延長

- 這個方案沒有什麼依據，就是執行24次（6天&times;4次/天）[add_lastHr.py](https://sinotec2.github.io/Focus-on-Air-Quality/utilities/netCDF/add_lastHr/)，將最後一天的變化重複6次。
- 因為CAMS濃度等級有一定的代表性，不致造成顯著高/低估的結果。

## 排放量預備程式之修正

- `nd=5` -> `nd=10`
- 新月份壓縮檔（.tar.gz, .tar.tx）的解壓縮
- 排放量的執行雖然不會佔據太多時間，此處還是將其提前到風場模式之前，安排在背景同步(批次)執行。

### 東亞與中國東南範圍排放量之預備

- REAS排放量部分較為單純，直接放到背景執行。

```bash
#kuang@master /nas1/backup/data/NOAA/NCEP/GFS/YYYY
#$ cat fcst.cs
...
#background executions of mk_emis and mk_ptse
for i in 0 1;do
  ii=$(echo ${GRD[$i]}|cut -c5-)
  cd $fcst/grid$ii/smoke
  ~/bin/sub ../../mk_emis.py $BEGD
done
~/bin/sub $gfs/em3.cs

~/bin/wait_exe metgrid #make sure all metgrid executions are finished
...
```

### mk_emis.py的修正

```bash
12a13
> tar =subprocess.check_output('which tar' ,shell=True).decode('utf8').strip('\n')
24c25
< nd=5
---
> nd=11
41a43,44
>   exts=['tar.xz',  'tar.gz',                    'tar.xz' ]
>   opts=['xvfJ',    'xvfz',                      'xvfJ'   ]
48a52,54
>     if not os.path.exists(fnames[i]):
>       tarfile=fnames[i].replace('ncf',exts[i])
>       os.system(tar+' '+opts[i]+' '+tarfile+'>&/dev/null')
87a94,95
```

### 臺灣地區排放量之預備

- 整體批次以背景、平行方式計算
- 批次檔內部則為循序執行
- em3.cs 內容

```bash
#cat $fcst/em3.cs
cd $fcst/grid03/smoke
../../mk_emis.py $BEGD
/usr/bin/ncks -O -d LAY,0 TEDS.ncf TEDS0.ncf
/usr/bin/ncatted -a NLAYS,global,o,i,1 TEDS0.ncf
./mk_ptse.py $BEGD
```

### mk_ptse.py的修正

- 將時間從5天(121小時)延長至10天(241小時)

```bash
kuang@master /nas2/cmaqruns/2022fcst/grid03/smoke
$ diff mk_ptseOld.py mk_ptse.py
11c11
< nts={'const':1,'timvr':121}
---
> nts={'const':1,'timvr':241}
36d35
<
```

## CMAQ平行運作之擴充

### 雙工作站平行運作方案

- 啟動DEV2加入平行運作，增加1倍節點數

網格|NPCOL_NPROW|NPROCS|machinefile|需時
:-:|:-:|:-:|:-:|:-:
grid45|11 16|176|<p>devp:88</p><p>dev2:88</p>|1:25
grid09|20 10|200|<p>devp:100</p><p>dev2:100</p>|4:00
grid03|8 23|184|<p>devp:88</p><p>dev2:88</p>|1:50

### 雙工作站循序執行方案_分半批次

- 增加跨網路節點似乎沒辦法減少太多執行時間，尤其是grid09。可能是這2個工作站的速度差異還頗大的，造成速度瓶頸所致。
- 此方案以dev2執行10天中的前半段、後半段與結果整合，則由速度較快的devp負責執行。
- dev2的啟動機制
  - 當上一層最後一天的PMs結果都處理好了，就開始執行下一層的`CCMS`與`run_bcon`。
  - 由於每一天的「最後一天」都會是原先不存在的新檔案，這項條件算是很明確。
  - 重複執行第2、3層的`CCTM`。
- devp的啟動機制
  - 只要dev2一開始執行`CCTM`、`ICON_yesterday_${DOM[$i]}`檔案就可以被覆蓋、可以同步進行下半段的`CCTM。
  - 判斷邏輯以最新的CCTM_ACONC檔名時間標籤(`$ymd`)，正好等於第一天(`${datep[0]}`)。
- 綜整2工作站的成果，準備下一層`CCTM`的執行。
  - 下一層各日BCON的切割納入平行運作，以其所有檔案大小的總和(`siz=$(du -ac ${f[@]})`)為判斷依據。
  - 如果總和正確，則進行`ncrcat`整併、就可以進行下一層`CCTM`的執行。

網格|NPCOL_NPROW|NPROCS|平行方案需時|循序方案需時
:-:|:-:|:-:|:-:|:-:
grid45|11 8|88|1:25|-
grid09|10 10|200|4:00|**2:20**
grid03|4 23|92|1:50|1:30

1. grid45並不使用循序方案
2. grid09有較大的差別、速度提升一倍。grid03略有提升，差異不大。
3. 第6天的初始濃度解決方案
   1. 昨天估計的`cgrd=$fcst/${GRD[$i]}/cctm.fcst/daily/CCTM_CGRID_v532_intel_${DOM[$i]}_${datep[5]}.nc`，與當天的`cgrd(${datep[4]})`，差異比想像的還大，可能grid09與grid03的6～10天還是需重算一遍。 】、
   1. 當天grid45第5天的CCTM_CGRID檔當成第6天grid09與grid03的初始濃度、1～10天的CCCTM_ACONC「也」作為grid03的邊界條件，好處是符合當天模擬風場、符合當天模擬的grid45條件，壞處是解析度不足。

### 循序跨解析度執行

- 由不同工作站各自[分半執行](#雙工作站循序執行方案_分半批次)，不但在起始日造成濃度驟變、在第5日也有跳動，失去執行10日的連續效果，似乎是本末倒置，失去初衷。
- 此處直接跳過東南大陸、另外執行較低解析度邊界條件的台灣範圍模擬，以求速效。2個工作站的執行批次分配如下表

#### 工作分配

順序|DEVP|dev2|說明
-|-|-|-
1|real/wrf|(real/wrf,受DEVP支配)|[雙工作站平行運作方案](#雙工作站平行運作方案)
2|CMAQ@CWBWRF_45k|等候|單機執行。等待DEVP完成10日之執行、並產生低解析度邊界檔案
3|CMAQ@SECN_9k範圍|執行低解析度邊界條件之CMAQ@TWEPA_3k|循序執行10日
4|(持續執行)|[等候、分日執行](#等候分日執行)高解析度邊界條件之CMAQ@TWEPA_3k|視DEVP執行進度配合執行

#### run_bcon13.csh及run_bcon.cs

- 顧名思義，這個批次檔不是循序產生邊界條件，而是直接從doman1的模擬結果，切出doman3的邊界條件。
- 畢竟TWEPA_3k範圍的排放量較REAS資料庫有明顯的高估情形，邊界條件影響不大，除了真正發生嚴重的境外事件。
- 因呼叫批次檔的設定有點亂，將這一段落分別出來，run_bcon13.csh/run_bcon.csh 2個批次檔以run_bcon.cs來呼叫，用引數方式來處理，會單純很多。
- run_bcon13.csh/run_bcon.csh 2個批次檔的差異

```bash
```

run_bcon.cs的內容

```bash
```

#### 等候執行低解析度邊界條件之暫時方案

- 判斷條件：東亞範圍執行到最後一天、且產生了domain3的邊界條件
- 如果檔案都具備了，就開始執行CMAQ@TWEPA_3k，並將結果輸出到網站上。

```bash
```

#### 等候、分日執行

- 逐日執行迴圈
- 判斷條件：當日的CCTM_CGRID檔案確實存在、並且是${datep[0]}所產生的(CDATE+CTIME)。

```bash
```

- 逐日執行需要：
  - 單日的邊界條件。由SECN_9k執行結果而來
  - 前日的CCTM_CGRID檔
  - 控制檔project.config，更改模版中的起訖日期與模擬範圍時數
- 執行結果進行combine（循序）
- 執行cmaq_jsonByDay(平行)

```bash
```

## fcst10.cs下載點

- {% include download.html content="10天版本空品預報之作業腳本[fcst10.cs](https://github.com/sinotec2/Focus-on-Air-Quality/blob/main/GridModels/ForecastSystem/fcst10.cs)" %}


[GFS]: <https://en.wikipedia.org/wiki/Global_Forecast_System> "全球預報系統 (GFS) 是一個全球數值天氣預報系統，包含由美國國家氣象局 (NWS) 運行的全球尺度氣象數值預報模式和變分分析。"
[fcst5]: <https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/ForecastSystem/> "逐日WRF與CMAQ預報系統之建置"
[CAMS]: <https://ads.atmosphere.copernicus.eu/cdsapp#!/dataset/cams-global-atmospheric-composition-forecasts?tab=overview> "CAMS每天2次進行全球大氣成分的5天預報，包括50多種氣狀物和7種顆粒物(沙漠塵埃、海鹽、有機物、黑碳、硫酸鹽、硝酸鹽和銨氣溶膠)。初始條件為衛星及地面觀測數據同化分析結果，允許在地面觀測數據覆蓋率低、或無法直接觀測到的大氣污染物進行估計，除此之外，它還使用到基於調查清單或觀測反衍的排放估計，以作為表面的邊界條件。"
[WACCM]: <https://www2.acom.ucar.edu/gcm/waccm> "The Whole Atmosphere Community Climate Model (WACCM) is a comprehensive numerical model, spanning the range of altitude from the Earth's surface to the thermosphere"
[ndown]: <https://sinotec2.github.io/Focus-on-Air-Quality/wind_models/REAL/ndown/> "將上層母網格結果，作為下層子網格的初始即邊界條件，所使用的讀取程式"
[ndowncs]: <https://github.com/sinotec2/Focus-on-Air-Quality/blob/main/GridModels/ForecastSystem/ndown.cs> "由大陸東南wrfout結果切割下層邊界條件之作業腳本"
[FDDA]: <https://zh.wikipedia.org/zh-tw/数据同化> "數據同化，或稱資料同化，是通過數學模型擬合觀測數據的一種漸進方式，通常用於複雜系統的建模和動態預報。"
[ungrib]: <https://sinotec2.github.io/Focus-on-Air-Quality/wind_models/WPS/namelist.wps/#再分析數據之轉檔> "再分析數據之轉檔"
[metgrid]: <https://sinotec2.github.io/Focus-on-Air-Quality/wind_models/WPS/namelist.wps/#metgridexe再分析數據之網格化> "metgrid.exe再分析數據之網格化"
[w1]: <https://sinotec2.github.io/Focus-on-Air-Quality/AQana/GAQuality/3WACCM/> "FAQ->AQ Data Analysis->Global AQ Data Analysis->WACCM模式結果之下載、讀取及應用"
[w2]: <https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/ForecastSystem/11WACCMasBCON/> "FAQ->CMAQ Model System->Forecast System->使用WACCM全球預報作為東亞邊界條件"
[cmaq_json]: <https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/ForecastSystem/9.join_nc/#cmaq_json3py版本> "初始時段濃度模擬結果之均勻化->cmaq_json3.py版本"

[^1]: https://sinotec2.github.io/FAQ/2022/12/01/wget_vs_curl.html " curl與wget下載之比較"
[^2]: https://sinotec2.github.io/FAQ/2022/08/10/GFStoWRF.html " GFS數據驅動WRF"
[^3]: https://sinotec2.github.io/FAQ/2022/08/05/earth_gfs.html " GFS數據自動下載轉換"
