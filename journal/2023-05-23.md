# Tuesday, May 23, 2023


## Support vector regression machines

### lecture of Dr. Saed Sayad

An Introduction to Data Science, [Dr. Saed Sayad(2022)](https://www.saedsayad.com/support_vector_machine_reg.htm)

![](https://www.saedsayad.com/images/SVR_5.png)

### 淺談聯合型用戶需量競價之用電預測技術 - 凌群

- [李凱平 2017](https://www.syscom.com.tw/ePaper_New_Content.aspx?id=634&EPID=241&TableName=sgEPArticle)
  > 要建立一個優良的預測模型，要藉由適當的特徵參數參與建模，以計算出表現最佳的模型參數。本文應用的電力資料與時間及溫度有極強的關聯性，而由於假日與非假日的耗能模式差異頗大，故在資料篩選方面，先將假日與非假日區別開來(如圖五)，然後個別建立每15分鐘的SVR模型，故共有96個非假日SVR模型以及96個假日SVR模型(共192個)。在資料輸入部分，非假日將以「當前這一小時之耗電量」為預測目標，而以「前24、48、72、96、120小時時段之耗電量」為特徵參數；假日部分同樣以「當前這一小時之耗電量」為預測目標，而以「前兩周末六日同一個小時時段及前一小時時段之耗電量」為特徵參數。為避免模型過度配適，本文將使用交互驗證的方式來避免，亦即將訓練資料分成多個子集合，分別對模型作訓練參數修正。

### medium.com

[資料分析&機器學習] 第3.4講：支援向量機(Support Vector Machine)介紹

[Yeh James2017](https://medium.com/jameslearningnote/資料分析-機器學習-第3-4講-支援向量機-support-vector-machine-介紹-9c6c6925856b)
> 虛線上的點X1,X2 其實就是所謂的支援向量(Support vector)，我們主要是利用支援向量來算出Margin，並最大化Margin。那要怎麼計算margin呢？ 利用高中數學的知識將X1向量-X2向量得到的向量投影到W就可以了！接下來就是在Y*(W*X) ≥k 的條件下(虛線中間沒有點)，來最大化margin 。

![](https://miro.medium.com/v2/resize:fit:784/format:webp/1*mIaoPZF2RC6oedLXcLRN_g.png)


### ithome

 全民瘋AI系列2.0系列 第 11 篇[Day 11] 核模型 - 支持向量機 (SVM) by [10程式中 2022](https://ithelp.ithome.com.tw/articles/10270447)
> 線性可分支持向量機就是在下圖範例的二維圖形中找出一條線，目標讓這條直線與兩個類別之間的間隔寬度距離最大化。其中離兩條虛線(間隔超平面)距離最近的點，就稱為支持向量 (support vector)。

![](https://ithelp.ithome.com.tw/upload/images/20210923/20107247rJUDvApGWA.png)

> SVR 迴歸器

> 支持向量機（SVM）是專門處理**分類**的問題，還有另一個名詞稱為支持向量迴歸（Support Vector Regression, SVR）專門處理迴歸問題。SVR 是 SVM 的延伸，而支持向量迴歸只要 f(x) 與 y 偏離程度不要太大，既可以認為預測正確。如下圖中的迴歸範例，在線性的 SVR 模型中會在左右加上 &epsilon; 作為模型容忍的區間。因此在訓練過程中只有在**虛線以外的誤差**才會被計算。此外 SVR 也提供了線性與非線性的核技巧，其中在非線性的模型中可以使用高次方轉換或是高斯轉換。

![](https://ithelp.ithome.com.tw/upload/images/20210923/20107247INMaCBNbi5.png)

## Likelihood

### Description of Maximum Likelihood Clustering

 Sharma, A., Shigemizu, D., Boroevich, K.A., López, Y., Kamatani, Y., Kubo, M., Tsunoda, T. (2016). Stepwise iterative maximum likelihood clustering approach. BMC Bioinformatics 17, 319. https://doi.org/10.1186/s12859-016-1184-5


> In general, the computation of first and second derivatives of likelihood is required to find the solution. If the likelihood is differentiable and the a priori probability is non-zero, then convergence can be obtained.

![](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-016-1184-5/MediaObjects/12859_2016_1184_Fig1_HTML.gif?as=webp)
> An illustration of stepwise iterative maximum likelihood method using a c = 2 cluster case. In this illustration, two clusters and are given with likelihood functions L1 and L2, respectively. The center of clusters are depicted by μ 1 and μ 2 (shown as ‘+’ inside two clusters). Initial total likelihood is Lold which is the sum of two likelihood functions (L1 + L2). A sample x∈ is checked for grouping. It is advantageous to shift sample x to cluster only if the new likelihood (Lnew = L *1  + L *2 ) is higher than the old likelihood; i.e., L new  > L old 
